{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333e17a",
   "metadata": {},
   "source": [
    "## Xray Lung Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf803e",
   "metadata": {},
   "source": [
    "---\n",
    "### Project flow  \n",
    "\n",
    "1) Importing the libraries and Loading the Images\n",
    "\n",
    "2) Exploring the Images and transforming the Images\n",
    "\n",
    "3) Creating the model Architecture\n",
    "\n",
    "4) Training the Data\n",
    "\n",
    "5) Evaluate the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d128145",
   "metadata": {},
   "source": [
    "---\n",
    "### Importing the libraries and Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the directory \n",
    "os.chdir('../artifacts/data_ingestion/chest_xray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the current working directory \n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Path\n",
    "data_path = '../chest_xray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd64a89",
   "metadata": {},
   "source": [
    "---\n",
    "### Exploring the Images and transforming the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the class name \n",
    "class_name = ['NORMAL','PNEUMONIA']\n",
    "\n",
    "# Creating a function to  get the list of files\n",
    "def get_list_of_files(dir_name):\n",
    "    '''\n",
    "    input - The input directory location\n",
    "    output - Returns the list the files in the directory\n",
    "    '''\n",
    "    files_list = os.listdir(dir_name)\n",
    "    return files_list\n",
    "files_list_normal_train = get_list_of_files(data_path+'/train/'+class_name[0])\n",
    "files_list_pneumonia_train = get_list_of_files(data_path+'/train/'+class_name[1])\n",
    "files_list_normal_test = get_list_of_files(data_path+'/test/'+class_name[0])\n",
    "files_list_pneumonia_test = get_list_of_files(data_path+'/test/'+class_name[1])\n",
    "\n",
    "\n",
    "    pr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ca2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train samples in Normal category {}\".format(len(files_list_normal_train)))\n",
    "print(\"Number of train samples in Pneumonia category {}\".format(len(files_list_pneumonia_train)))\n",
    "print(\"Number of test samples in Normal category {}\".format(len(files_list_normal_test)))\n",
    "print(\"Number of test samples in Pneumonia category {}\".format(len(files_list_pneumonia_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4a3c3",
   "metadata": {},
   "source": [
    "---\n",
    "### Exploring the images \n",
    "\n",
    "- Let's print the Normal and Pneumonia images from Training folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188baa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_train))\n",
    "img = data_path + '/train/NORMAL/'+ files_list_normal_train[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/train/PNEUMONIA/'+ files_list_pneumonia_train[np.random.randint(0,len(files_list_pneumonia_train))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706101e6",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "- If we run the above cell mutiple times we can see that the images are of different shapes for the 'NORMAL' and 'PNEUMONIA' images in the **train** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_test))\n",
    "img = data_path + '/test/NORMAL/'+ files_list_normal_test[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0658cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/test/PNEUMONIA/'+ files_list_pneumonia_test[np.random.randint(0,len(files_list_pneumonia_test))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecb4f5",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "- If we run the above cell mutiple times we can see that the images are of different shapes for the 'NORMAL' and 'PNEUMONIA' images in the **test** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f56bd",
   "metadata": {},
   "source": [
    "### Transforming the Images \n",
    "\n",
    "- Now that we have seen the sample of the images let's transform the data now \n",
    "- We need to perform transformation on both train and test images \n",
    "- For Training data we need to perform the data augmentation also.\n",
    "- Data Augmentation is done to create synthetic data.\n",
    "\n",
    "In **Transformation** we are doing Resize,CenterCrop,ColorJitter,RandomHorizontalFlip,RandomRotation,ToTensor and Normalize.\n",
    "\n",
    "- Resize:- Resize the input image to the given size.\n",
    "- CenterCrop:- Crops the given image at the center.\n",
    "- ColorJitter:- Randomly change the brightness, contrast, saturation and hue of an image.\n",
    "- RandomHorizontalFlip:- Horizontally flip the given image randomly with a given probability.\n",
    "- RandomRotation:- Rotate the image by angle.\n",
    "- ToTensor:- Convert numpy.ndarray to tensor.\n",
    "- Normalize:- Normalize a float tensor image with mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60b8f6",
   "metadata": {},
   "source": [
    "### Creating Data Loader\n",
    "\n",
    "- For our usecase will be using the default data loader for Pytorch.\n",
    "- We will be creating 2 data loaders one for the training data and other for the test data.\n",
    "- batch size is a hyperparameter which we can tweak according to our need and system configuration.\n",
    "- We can provide Image shuffling True for training data and False for test data.\n",
    "- Pin memory is used to transfer the loaded dataset from CPU to GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2249a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(os.path.join(data_path, 'train'), transform= train_transform)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_path, 'test'), transform= test_transform)\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size= 2, shuffle= True, pin_memory= True)\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size= 2, shuffle= False, pin_memory= True)\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f'Number of train images: {len(train_data)}')\n",
    "print(f'Number of test images: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d92982",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating the model Architecture\n",
    "\n",
    "- First Layer is the **input layer** consisting of 3 input channels and output channels with kernel_size of 3X3, padding=0 and bias=True. The activation function we are using is ReLU and performing batch normalization.\n",
    "- Then we are performing max pooling to extract the important features out of the image.\n",
    "- Similarly we are passing our model through 9 convolutional layers.\n",
    "- Finally we are passing out passing our model through a output layer in which we are getting binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be648955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creating custom CNN architecture for Image classification\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # Input Block\n",
    "        self.convolution_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "        self.pooling11 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convolution_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=20, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.pooling22 = nn.MaxPool2d(2, 2)\n",
    "        self.convolution_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10),\n",
    "        )\n",
    "        self.pooling33 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convolution_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.convolution_block5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "#         self.convblock6 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(10),\n",
    "#         )\n",
    "        self.convolution_block6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "#         self.convblock8 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(32)\n",
    "#         )\n",
    "        self.convolution_block7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "        self.convolution_block8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(14)\n",
    "        )\n",
    "        self.convolution_block9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        # OUTPUT BLOCK\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.convolution_block_out = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(4, 4), padding=0, bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_block1(x)\n",
    "        x = self.pooling11(x)\n",
    "        x = self.convolution_block2(x)\n",
    "        x = self.pooling22(x)\n",
    "        x = self.convolution_block3(x)\n",
    "        x = self.pooling33(x)\n",
    "        x = self.convolution_block4(x)\n",
    "        x = self.convolution_block5(x)\n",
    "#         x = self.convblock6(x)\n",
    "        x = self.convolution_block6(x)\n",
    "#         x = self.convblock8(x)\n",
    "        x = self.convolution_block7(x)\n",
    "        x = self.convolution_block8(x)\n",
    "        x = self.convolution_block9(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.convolution_block_out(x)\n",
    "        x = x.view(-1, 2)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d63c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check weather cuda is available in the system or not \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Available processor {}\".format(device))\n",
    "model = Net().to(device)\n",
    "# To check the model summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33b98d",
   "metadata": {},
   "source": [
    "---\n",
    "### Training the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc13718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Description: To train the model \n",
    "    \n",
    "    input: model,device,train_loader,optimizer,epoch \n",
    "    \n",
    "    output: loss, batch id and accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Initialization of gradient\n",
    "        optimizer.zero_grad()\n",
    "        # In PyTorch, gradient is accumulated over backprop and even though thats used in RNN generally not used in CNN\n",
    "        # or specific requirements\n",
    "        ## prediction on data\n",
    "        y_pred = model(data)\n",
    "        # Calculating loss given the prediction\n",
    "        loss = F.nll_loss(y_pred, target)\n",
    "        train_losses.append(loss)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # get the index of the log-probability corresponding to the max value\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Description: To test the model\n",
    "    \n",
    "    input: model, device, test_loader\n",
    "    \n",
    "    output: average loss and accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the params for training \n",
    "model =  Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "scheduler = StepLR(optimizer, step_size=6, gamma=0.5)\n",
    "EPOCHS = 4\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    print('current Learning Rate: ', optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99775265",
   "metadata": {},
   "source": [
    "### Evaluate the Model \n",
    "\n",
    "- Plotting the graph for taining loss and training accuracy\n",
    "- Plotting the graph for test loss and test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96710f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses1 = [float(i.cpu().detach().numpy()) for i in train_losses]\n",
    "train_acc1 = [i for i in train_acc]\n",
    "test_losses1 = [i for i in test_losses]\n",
    "test_acc1 = [i for i in test_acc]\n",
    "fig, axs = plt.subplots(2,2,figsize=(16,10))\n",
    "axs[0, 0].plot(train_losses1)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[1, 0].plot(train_acc1)\n",
    "axs[1, 0].set_title(\"Training Accuracy\")\n",
    "axs[0, 1].plot(test_losses1)\n",
    "axs[0, 1].set_title(\"Test Loss\")\n",
    "axs[1, 1].plot(test_acc1)\n",
    "axs[1, 1].set_title(\"Test Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02249770",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "- As we can see from the above graph that training loss has many fluctuation at the start but then it consolidates over a period of time.\n",
    "- From the training accuracy graph we are able to get a training accuracy of around 50% throughout\n",
    "- Test loss graph decreases as the number of epochs increases.\n",
    "- test accuracy remains constant at around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc21f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f6046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 | packaged by conda-forge | (default, Nov 10 2021, 13:20:59) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "66766a64ec82f29787ee1d9fce2e5d5e0014bea6c686b522cfa9ebc0d7a8e070"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
